{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Algoritmo para analisis de componentes principales","metadata":{}},{"cell_type":"code","source":"from cmath import sqrt\nimport numpy as np\nimport numpy.linalg as alg\n\nfrom sklearn.decomposition import PCA\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport pandas as pd\n\nnp.set_printoptions(precision=6, suppress=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T06:00:55.380994Z","iopub.execute_input":"2022-04-24T06:00:55.381390Z","iopub.status.idle":"2022-04-24T06:00:55.388082Z","shell.execute_reply.started":"2022-04-24T06:00:55.381351Z","shell.execute_reply":"2022-04-24T06:00:55.387049Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# La siguiente clase implementa el algoritmo de analisis de componentes principales\n\n## Para la implementación del algoritmo se siguen los siguientes pasos:\n\n* Centrar y reducir los datos\n* Calcular la matriz de correlación\n* Calcular los valores y vectores propios de la matriz de correlación\n* Ordenar los vectores propios basado en la magnitud de sus valores propios asociados\n* Selección de las n columnas utilizadas como componente principal\n\nA Este punto se pueden calcular metricas para la eficiencia del algoritmo, como el vector de inercia que representa el porcentaje de importancia para cada columna de la transformación obtenida.","metadata":{}},{"cell_type":"code","source":"class My_PCA:\n    def __init__(self, n_components = -1):\n        self.n_components = n_components\n        self.matrix = []\n        self.inertia = []\n        self.points = []\n        self.eigenvalues = []\n        self.eigenvectors = []\n    pass\n\n    def fit(self, X):\n        redux = self.center_and_scale(X)\n        corr = self.corr_matrix(redux)\n        self.eigenvalues, self.eigenvectors = self.eigensomething(corr)\n        self.matrix = self.pca_matrix(self.eigenvectors)\n        self.remove_components()\n        self.inertia = self.calculate_inertia(self.eigenvalues)\n        self.points = self.calculate_points(self.eigenvalues)\n        pass\n\n\n    def transform(self, X):\n        return np.matmul(X, self.matrix)\n\n\n    def center_and_scale(self, X):\n        _X = X      \n\n        mmean = np.mean(_X, axis=0)\n        mstd = np.std(_X, axis=0)\n\n        _X = (_X - mmean) / mstd\n        return _X\n\n\n    def corr_matrix(self, X):\n        n, m = X.shape\n        return (1/n) * np.matmul(X.transpose(), X)\n\n    \n    def eigensomething(self, R):\n        w, v = alg.eigh(R)\n        sort_index = np.argsort(abs(w))[::-1]\n        sorted_eigenvals = w[sort_index]\n        sorted_eigenvecs = v[:, sort_index]\n        \n        return (sorted_eigenvals, sorted_eigenvecs)\n\n\n    def pca_matrix(self, vectors):\n        return np.array(vectors)\n\n\n    def remove_components(self):\n        self.matrix = np.delete(self.matrix, np.s_[self.n_components - 1 : -1], axis = 1)\n        self.eigenvalues = self.eigenvalues[0:self.n_components]\n        pass\n\n    def calculate_inertia(self, eigenvalues):\n        n, m = self.matrix.shape\n        return eigenvalues / m\n\n    \n    def calculate_points(self, eigenvalues):\n        w0 = eigenvalues[0]\n        w1 = eigenvalues[1]\n        v0 = self.matrix[:,0]\n        v1 = self.matrix[:,1]\n        \n        return ((v0 * sqrt(w0)).real, (v1 * sqrt(w1)).real)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:55.439268Z","iopub.execute_input":"2022-04-24T06:00:55.439686Z","iopub.status.idle":"2022-04-24T06:00:55.458275Z","shell.execute_reply.started":"2022-04-24T06:00:55.439637Z","shell.execute_reply":"2022-04-24T06:00:55.457558Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Poniendo a prueba la imlementación\n## Se importará el dataset del Titanic para analizar cuales de sus caracteristicas pueden ser de interes","metadata":{}},{"cell_type":"code","source":"# Importación del dataset\nbase_dataset = pd.read_csv(\"../input/test-file/tested.csv\")\nprint(base_dataset.head())\n\n# Busqueda de valores ausentes\npercent_missing = base_dataset.isnull().sum() * 100 / len(base_dataset)\nmissing_value_df = pd.DataFrame({'column_name': base_dataset.columns,\n                                 'percent_missing': percent_missing})\n\nprint(missing_value_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:55.485382Z","iopub.execute_input":"2022-04-24T06:00:55.485894Z","iopub.status.idle":"2022-04-24T06:00:55.512387Z","shell.execute_reply.started":"2022-04-24T06:00:55.485837Z","shell.execute_reply":"2022-04-24T06:00:55.511588Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Información basica\nPodemos observar que hay datos utiles para ubicar a cada uno de los pasajeros, sin embargo son datos que no serán utiles para la clasificación de si este pasajero sobrevive o no, por lo que las columnas \"PassengerId\", \"Name\", \"Ticket\" ser completamente eliminadas, tambien podemos notar que la columna de Cabina tiene un 77% de información faltante por lo que podemos eliminarla completamente.\n\n## Codificación de información nominal\nLas clases \"Sex\", \"Embarked\" y \"Survived\" se pueden codificar con la tecnica de One Hot Encoding para facilitar el proceso de clasificación a futuro, la variable \"Pclass\" puede ser más compleja de tratar ya que es una variable ordinal, sin embargo no es una variable continua por lo que será mejor evitar la complejidad de esta variable y codificarla igualmente.","metadata":{}},{"cell_type":"code","source":"# Eliminación de variables innecesarias\ntitanic_data = base_dataset.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis=1)\ntitanic_data.dropna(axis=0, how=\"any\", inplace=True)\n\n# One hot encoding\ncnames = [\"Survived\", \"Pclass\", \"Sex\", \"Embarked\"]\nfor cname in cnames:\n    dummies = pd.get_dummies(titanic_data[cname], prefix=cname)\n    titanic_data = titanic_data.drop(cname, axis=1)\n    titanic_data = titanic_data.join(dummies)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:55.524354Z","iopub.execute_input":"2022-04-24T06:00:55.524915Z","iopub.status.idle":"2022-04-24T06:00:55.551258Z","shell.execute_reply.started":"2022-04-24T06:00:55.524859Z","shell.execute_reply":"2022-04-24T06:00:55.550126Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Preparación de los datos para utilizar PCA\nPara poder procesar los datos con el algoritmo de PCA es necesario transformar los datos de un Pandas dataset a una matriz numerica de NumPy.\n\nEn la implementación de PCA propia se considera que los datos serán centrados y reducidos dentro de la función de ´fit´ sin embargo al utilizar PCA implementado por sklearn los datos deben ser centrados y reducidos antes de ser introducidos en el algoritmo de PCA.","metadata":{}},{"cell_type":"code","source":"#   Conversion de datos a numpy matrix\ntitanic_as_np = titanic_data.to_numpy()\n\n#   Uso del metodo PCA implementado\nmypca = My_PCA(n_components=4)\nmypca.fit(titanic_as_np)\n\n#   Uso del metodo PCA de la biblioteca sklearn\npca = PCA(n_components=4)\n\n# Centrado y reducido previo al uso de sklearn\ntitanic_prep = mypca.center_and_scale(titanic_as_np)\npca.fit(titanic_prep)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:55.553826Z","iopub.execute_input":"2022-04-24T06:00:55.554528Z","iopub.status.idle":"2022-04-24T06:00:55.573507Z","shell.execute_reply.started":"2022-04-24T06:00:55.554470Z","shell.execute_reply":"2022-04-24T06:00:55.572467Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Uso del algoritmo de PCA sobre los datos originales\nPara modificar la matriz de datos originales solo es necesario centrar y reducir los datos, luego multiplicar la matríz resultante por la matríz obtenida con el algoritmo de PCA","metadata":{}},{"cell_type":"code","source":"# titanic_prep se encuentra centrada y reducida del bloque de codigo anterior\n\n# Transformación de los datos a partir del PCA implementado  \nmy_matrix = mypca.transform(titanic_prep)\n\n# Transformacion de los daatos a partir del PCA sklearn\nsk_matrix = pca.transform(titanic_prep)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:55.575840Z","iopub.execute_input":"2022-04-24T06:00:55.576803Z","iopub.status.idle":"2022-04-24T06:00:55.584318Z","shell.execute_reply.started":"2022-04-24T06:00:55.576740Z","shell.execute_reply":"2022-04-24T06:00:55.583146Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Graficación de los resultados\nPodemos ver como los datos se agrupan en 2 categorías princiales, sin embargo existe un tercer grupo donde los datos se encuentran mezclados, aún así este grupo es menor a los 2 grupos principales, esto se logra al transformar las coordenadas de cada punto aumentando al maximo la varianza entre las clases.","metadata":{}},{"cell_type":"code","source":"transformed_sk_data = pd.DataFrame({'F1' : sk_matrix[:, 0], 'F2': sk_matrix[:, 1], 'Survival': titanic_data['Survived_0']})\ntransformed_my_data = pd.DataFrame({'F1' : my_matrix[:, 0], 'F2': my_matrix[:, 1], 'Survival': titanic_data['Survived_0']})\nsns.set()\n\nfig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,5))\nfig.suptitle('Comparación de puntos con PCA')\n\nsns.scatterplot(ax=axes[0], x=\"F1\", y=\"F2\", hue='Survival', data=transformed_sk_data)\naxes[0].set_title('Datos aplicando PCA de sklearn')\n\nsns.scatterplot(ax=axes[1], x=\"F1\", y=\"F2\", hue='Survival', data=transformed_my_data)\naxes[0].set_title('Datos aplicando mi PCA')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:55.587142Z","iopub.execute_input":"2022-04-24T06:00:55.588234Z","iopub.status.idle":"2022-04-24T06:00:56.257695Z","shell.execute_reply.started":"2022-04-24T06:00:55.588175Z","shell.execute_reply":"2022-04-24T06:00:56.256480Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Inercia e importancia de caracteristicas\nA partir de la inercia podemos saber que tan importante será cada columna de datos final, pero tambien podemos obtener esta información de manera grafica dentro de un circulo de correlación, esto nos dirá cuanta correlación o \"importancia\" tendrá cada columna para obtener un resultado.\n\nPara este caso podemos ver que las variables con una correlación más fuerte son:\n* Supervivencia\n* Sexo\n* Clase\n* Donde ha embarcado\n* Tarifa\n* Edad","metadata":{}},{"cell_type":"code","source":"x, y = mypca.points\n\n(fig, ax) = plt.subplots(figsize=(10, 10))\nfor i in range(0, len(titanic_data.columns)):\n    ax.arrow(0, 0,\n             x[i],y[i],\n             head_width=0.1,head_length=0.1)\n    plt.text(x[i],y[i],titanic_data.columns[i])\n \nan = np.linspace(0, 2 * np.pi, 100)\nplt.plot(np.cos(an), np.sin(an))\nplt.axis('equal')\nax.set_title('Circulo de correlación')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T06:00:56.259750Z","iopub.execute_input":"2022-04-24T06:00:56.260161Z","iopub.status.idle":"2022-04-24T06:00:56.609876Z","shell.execute_reply.started":"2022-04-24T06:00:56.260112Z","shell.execute_reply":"2022-04-24T06:00:56.608768Z"},"trusted":true},"execution_count":39,"outputs":[]}]}