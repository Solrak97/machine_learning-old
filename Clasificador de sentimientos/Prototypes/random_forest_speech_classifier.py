# -*- coding: utf-8 -*-
"""Random forest - speech classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HXy-Ps84uzRkZO0tHdPDr1LOK_bEDXxm

# Busqueda de entradas para el modelo base propuesto por Dias Issa et al.
El modelo base cuenta con una entrada de 193 nodos, por lo que es necesario que cada entrada de datos tenga exactamente 193 variables, eso a partir de las ya conocidas caracteristicas 

* Chromagram
* Contrast
* Mel coeficent
* Mel frequency
* Tonnetz

### Descarga de los datos desde Kaggle hasta el notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
#%%capture
# ! pip install kaggle wavio pydub keras-metrics
# 
# #! rm -rf *
# #! mkdir ~/.kaggle
# #! curl https://raw.githubusercontent.com/Solrak97/clasificador_de_sentimientos/main/kaggle.json > kaggle.json
# #! cp kaggle.json ~/.kaggle/
# #! chmod 600 ~/.kaggle/kaggle.json
# #! kaggle datasets download uwrfkaggler/ravdess-emotional-speech-audio
# #! unzip ravdess-emotional-speech-audio.zip
# #%%capture
# ! curl https://raw.githubusercontent.com/Solrak97/clasificador_de_sentimientos/main/data.pkl > data.pkl
# 

'''
Trabajando con el ambiente:
  cd Prototypes
  python3 -m venv prot_env
  cd ..
  .\Prototypes\prot_env\Scripts\activate
  pip -r .\Prototypes\requirements.pip   
'''
#import soundfile
import numpy as np
import pandas as pd
#import librosa
import glob
import os
import pickle
#from pydub import AudioSegment
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

import wandb
from torchmetrics import F1Score, Recall, Precision, Accuracy

sweep_config = {
    "name": "Sweep Analisis Sentimientos",
    "method": "random",
    "parameters": {
        "n_estimators": {
            "values": [100, 200, 700]
        },

        "model": {
            "values": ["Tree"]
        }
    }
}

sweep_id = wandb.sweep(sweep_config)

count = 100
wandb.agent(sweep_id, function=train, count=count)



data = pd.read_pickle('data.pkl')

'''
Graficación de matríz de confusión
'''
def confussion_matrix(y_true, y_pred, classes):

    cf_matrix = metrics.confusion_matrix(y_true, y_pred)

    df_cm = pd.DataFrame(cf_matrix, index=[i[0] for i in classes],
                         columns=[i[0] for i in classes])

    plt.figure(figsize=(10, 7))
    sns.heatmap(df_cm, annot=True, fmt='g', cmap='YlOrBr')

    plt.show()

mean_accuracy_200 = 0
mean_accuracy_400 = 0
mean_accuracy_600 = 0
mean_accuracy_800 = 0
mean_accuracy_1000 = 0
'''
Construcción de la matriz de resultados por corrida
  model_performance[0] --> n_estimators
  model_performance[1] --> f1
  model_performance[2] --> acurracy
  model_performance[3] --> precision
  model_performance[4] --> recall
  k_runs               --> cantidad de corridas para cada combinación
  n_estimators         --> lista con el valor de tal hiperparámetro
'''
k_runs = 10
n_estimators = np.repeat([200, 400, 600, 800, 1000] , k_runs).tolist()
model_performance = [n_estimators, [], [], [], []]
runs = len(model_performance[0])
#runs = 15

'''
Corriendo los modelos k veces para cada combinación de hiperparámetros
'''
for run in range(runs):
  '''
  Construyendo a X
  '''
  ##############X = np.array([np.hstack([data['Duration'][i], data['MFCC'][i], data['Chroma'][i], data['Mel'][i], 
                          ############data['Contrast'][i], data['Tonnetz'][i]]) for i in range(len(data.index))])
  X = np.array([np.hstack([data['Duration'][i], data['MFCC'][i], data['Chroma'][i], data['Mel'][i]]) for i in range(len(data.index))])

  '''
  Construyendo a Y
  '''
  Y = np.array(data['Ordinal_Emotion'])
  classes = np.array(data['Emotion'])
  classes = pd.DataFrame(data=classes)

  '''
  Construyendo los datasets de entrenamiento y de testeo
  '''
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)

  '''
  Model: Random Forest
  '''
  estimators = model_performance[0][run]
  classifier = RandomForestClassifier(n_estimators = estimators, random_state = 0)
  c_p = classifier.fit(X_train, y_train).predict(X_test)
  print(f'\n\n...run N°{run+1}')
  print(f'...n_estimators {estimators}')

  accuracy = accuracy_score(y_true=y_test, y_pred=c_p)
  print(f'\nMy accuracy: {accuracy}')
  model_performance[2].append(accuracy)
  
  if model_performance[0][run] == 200:
    mean_accuracy_200 += accuracy
  elif model_performance[0][run] == 400:
    mean_accuracy_400 += accuracy
  elif model_performance[0][run] == 600:
    mean_accuracy_600 += accuracy
  elif model_performance[0][run] == 800:
    mean_accuracy_800 += accuracy
  elif model_performance[0][run] == 1000:
    mean_accuracy_1000 += accuracy

  '''Matriz de Confusión:
  report = classification_report(y_test, c_p)
  print(f'\n{report}')
  c_matrix = confusion_matrix(y_test, c_p)
  print(f'\n{c_matrix}')
  print(classes)

  df_cm = pd.DataFrame(c_matrix, index = [i for i in "ABCDEFGH"],
                    columns = [i for i in "ABCDEFGH"])
  plt.figure(figsize = (10,7))
  sns.heatmap(df_cm, annot=True)
  '''
  
  #sn.heatmap(df_cm, annot=True, annot_kws={"size": 16}) # font size
  ##############################plt.show()
mean_accuracy_200 /= k_runs
mean_accuracy_400 /= k_runs
mean_accuracy_600 /= k_runs
mean_accuracy_800 /= k_runs
mean_accuracy_1000 /= k_runs
print(f'------------------------------------------------')
print(f'El accuracy promedio por modelo fue de:')
print(f'\nn_estimators   acurracy')
print(f'\n200            {mean_accuracy_200}')
print(f'\n400            {mean_accuracy_400}')
print(f'\n600            {mean_accuracy_600}')
print(f'\n800            {mean_accuracy_800}')
print(f'\n1000           {mean_accuracy_1000}')
print(f'------------------------------------------------')





"""Precision 0.7 (100 estimators) y 0.647 (20 estimators)

Emociones consideradas: neutral, calm, happy, sad, angry, fearful, disgust y surprised.

Features consideradas: mfcc, chroma, mel, contrast y tonnetz.

Considerando solo mfcc, chorma y mel con las mismas emociones: 0.7 y 0.7

Considerando solo las emociones angry, sad, neutral y happy con los features mfcc, chroma, mel, contrast y tonnetz: 0.9 y 0.9

Considerando solo las emociones angry, sad, neutral y happy con los features mfcc, chroma y mel: 0.9 y 0.8

Parece que disminución de emociones y aumento de features aumentan precisión.
"""